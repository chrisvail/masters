{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreation of Student-Teacher Framework Paper\n",
    "\n",
    "This notebook attempts to recreate the results shown in the paper `Teacher-Student Framework: a Reinforcement Learning Approach` [1].\n",
    "\n",
    "We use the OpenAIGym MountainCar environment for this purpose, as is done in the paper. The agents use the SARSA algorithm with linear approximation functions and binary tile coding. As we get further into the implementation we can try to understand what exactly that means. But to start with we are going to just get on with it.\n",
    "\n",
    "[1] Matthieu Zimmer, Paolo Viappiani, Paul Weng. Teacher-Student Framework: a Reinforcement Learning Approach. AAMAS Workshop Autonomous Robots and Multirobot Systems, May 2014, Paris,\n",
    "France. ffhal-01215273f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create the environment and explore the space\n",
    "\n",
    "Pretty simple block of code here just creates the environment and renders out 500 steps using a random policy. Obviously nothings going to happen here so we can ignore the `done` flag. Then we take an observation and look at the action and observation space.\n",
    "\n",
    "Action space:\n",
    "\n",
    "    Size = 3\n",
    "    Actions = forwards, neutral, backwards\n",
    "\n",
    "Observation space: \n",
    "    \n",
    "    Size = 2\n",
    "    Observations = x position, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-0.55384094 -0.00470186]\n",
      "Action space:\tDiscrete(3)\n",
      "Observation space:\tBox([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    \n",
    "print(f\"Observation: {observation}\")\n",
    "env.reset()\n",
    "print(f\"Action space:\\t{env.action_space}\")\n",
    "print(f\"Observation space:\\t{env.observation_space}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the SARSA algorithm\n",
    "\n",
    "Here we set up the SARSA algorithm, defining the required parameters, setting action selection functions and update functions. This can then be used to train a model in future stages.\n",
    "\n",
    "We are using the $\\epsilon$-greedy strategy whereby random actions are chosen with a small probability (1 - $\\epsilon$) while all other times we take the on policy action. Currently there is no epsilon scheduling planned for the training loop however this could be built in quite easily.\n",
    "\n",
    "So theres still quite a bit to be done on this step but we are going to ignore that for now. Mainly, chosing a method for representing the q values is the problem. If I use a tabular form its quite simple to adjust and select. However the paper does use a linear approximation function which I need to understand better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 500\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    "\n",
    "def select_action(state):\n",
    "    if np.random.uniform(0, 1) > epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = get_best_action(state)\n",
    "\n",
    "    return action \n",
    "\n",
    "def get_best_action(state):\n",
    "    # TODO: Decide on how this function is going to work. It depends heavily on the function used for learning\n",
    "    return 1\n",
    "\n",
    "def update(state, action, reward, state2, action2):\n",
    "    q_val = get_q_val(state, action)\n",
    "    future_q = get_q_val(state2, action2)\n",
    "    new_q_val = q_val + alpha*(reward + gamma*future_q - q_val)\n",
    "    update_q_value(new_q_val)\n",
    "\n",
    "def get_q_val(state, action):\n",
    "    # Again requires me to decide on the specific q value function\n",
    "    return 1\n",
    "\n",
    "def update_q_value(new_value):\n",
    "    # See above\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Heres where we actually put the meat on the bones of the system. Training *should* be easy but we all know what happens when we write something like that down dont we? So anyway, lets keep it super simple to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "\n",
    "    # Reset to initial state\n",
    "    state1 = env.reset()\n",
    "    action1 = select_action(state1)\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "\n",
    "        # Take action\n",
    "        state2, reward, done, info = env.step(action1)\n",
    "        action2 = select_action(state2)\n",
    "\n",
    "        reward = 0 if done else -1\n",
    "\n",
    "        # Update Q values\n",
    "        update(state1, action1, reward, state2, action2)\n",
    "\n",
    "        # Prep for next step\n",
    "\n",
    "        state1, action1 = state2, action2\n",
    "\n",
    "        if done: break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4d31db5a4f04e443ebd72f8bffbbe859acbb5a34c8d8fe948f1961183823e52"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
